{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsJCP/4jrw+HtUYzy/8xmZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkAhNFxYSYqx",
        "outputId": "3f971735-4a1a-44c1-8843-8c196f3458ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.2.28-py3-none-any.whl (284 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache (from pyautogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker (from pyautogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flaml (from pyautogen)\n",
            "  Downloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.25.2)\n",
            "Collecting openai>=1.3 (from pyautogen)\n",
            "  Downloading openai-1.32.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pyautogen) (24.0)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.7.3)\n",
            "Collecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.4.0)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->pyautogen) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.3->pyautogen)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.18.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2024.5.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen) (3.3.2)\n",
            "Installing collected packages: python-dotenv, h11, flaml, diskcache, tiktoken, httpcore, docker, httpx, openai, pyautogen\n",
            "Successfully installed diskcache-5.6.3 docker-7.1.0 flaml-2.1.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.32.0 pyautogen-0.2.28 python-dotenv-1.0.1 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyautogen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = ''\n",
        "\n",
        "# Add your utilities or helper functions to this file.\n",
        "\n",
        "# import os\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# # these expect to find a .env file at the directory above the lesson.                                                                                                                     # the format for that file is (without the comment)                                                                                                                                       #API_KEYNAME=AStringThatIsTheLongAPIKeyFromSomeService\n",
        "# def load_env():\n",
        "#     _ = load_dotenv(find_dotenv())\n",
        "\n",
        "# def get_openai_api_key():\n",
        "#     load_env()\n",
        "#     openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "#     return openai_api_key\n",
        "\n",
        "\n",
        "llm_config = {\"model\": \"gpt-3.5-turbo\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3tn5FTDU5ni",
        "outputId": "3fea362f-bb1e-4e93-8ac0-d05c4dd98347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onboarding_personal_information_agent = ConversableAgent(\n",
        "    name=\"Onboarding Personal Information Agent\",\n",
        "    system_message='''You are a helpful customer onboarding agent,\n",
        "    you are here to help new customers get started with our product.\n",
        "    Your job is to gather customer's name and location.\n",
        "    Do not ask for other information. Return 'TERMINATE'\n",
        "    when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "J8L9rUrKU9qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onboarding_topic_preference_agent = ConversableAgent(\n",
        "    name=\"Onboarding Topic preference Agent\",\n",
        "    system_message='''You are a helpful customer onboarding agent,\n",
        "    you are here to help new customers get started with our product.\n",
        "    Your job is to gather customer's preferences on news topics.\n",
        "    Do not ask for other information.\n",
        "    Return 'TERMINATE' when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "nDIPIzsWVFrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_engagement_agent = ConversableAgent(\n",
        "    name=\"Customer Engagement Agent\",\n",
        "    system_message='''You are a helpful customer service agent\n",
        "    here to provide fun for the customer based on the user's\n",
        "    personal information and topic preferences.\n",
        "    This could include fun facts, jokes, or interesting stories.\n",
        "    Make sure to make it engaging and fun!\n",
        "    Return 'TERMINATE' when you are done.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
        ")"
      ],
      "metadata": {
        "id": "o3iJMfrJVIes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_proxy_agent = ConversableAgent(\n",
        "    name=\"customer_proxy_agent\",\n",
        "    llm_config=False,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
        ")"
      ],
      "metadata": {
        "id": "24MhL_YxVMoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chats = [\n",
        "    {\n",
        "        \"sender\": onboarding_personal_information_agent,\n",
        "        \"recipient\": customer_proxy_agent,\n",
        "        \"message\":\n",
        "            \"Hello, I'm here to help you get started with our product.\"\n",
        "            \"Could you tell me your name and location?\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_args\": {\n",
        "            \"summary_prompt\" : \"Return the customer information \"\n",
        "                             \"into as JSON object only: \"\n",
        "                             \"{'name': '', 'location': ''}\",\n",
        "        },\n",
        "        \"max_turns\": 2,\n",
        "        \"clear_history\" : True\n",
        "    },\n",
        "    {\n",
        "        \"sender\": onboarding_topic_preference_agent,\n",
        "        \"recipient\": customer_proxy_agent,\n",
        "        \"message\":\n",
        "                \"Great! Could you tell me what topics you are \"\n",
        "                \"interested in reading about?\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"max_turns\": 1,\n",
        "        \"clear_history\" : False\n",
        "    },\n",
        "    {\n",
        "        \"sender\": customer_proxy_agent,\n",
        "        \"recipient\": customer_engagement_agent,\n",
        "        \"message\": \"Let's find something fun to read.\",\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "qyZl__4MVPav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import initiate_chats\n",
        "\n",
        "chat_results = initiate_chats(chats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-B_CyO2VTMs",
        "outputId": "c65fd78b-cdce-4799-bfb0-33f62cf52e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Onboarding Personal Information Agent (to customer_proxy_agent):\n",
            "\n",
            "Hello, I'm here to help you get started with our product.Could you tell me your name and location?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/autogen/agentchat/chat.py:47: UserWarning: Repetitive recipients detected: The chat history will be cleared by default if a recipient appears more than once. To retain the chat history, please set 'clear_history=False' in the configuration of the repeating agent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provide feedback to Onboarding Personal Information Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: bob\n",
            "customer_proxy_agent (to Onboarding Personal Information Agent):\n",
            "\n",
            "bob\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Onboarding Personal Information Agent (to customer_proxy_agent):\n",
            "\n",
            "Thank you for providing your name. Could you also please tell me your location?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to Onboarding Personal Information Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: new york\n",
            "customer_proxy_agent (to Onboarding Personal Information Agent):\n",
            "\n",
            "new york\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Onboarding Topic preference Agent (to customer_proxy_agent):\n",
            "\n",
            "Great! Could you tell me what topics you are interested in reading about?\n",
            "Context: \n",
            "{'name': 'bob', 'location': 'new york'}\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to Onboarding Topic preference Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: food\n",
            "customer_proxy_agent (to Onboarding Topic preference Agent):\n",
            "\n",
            "food\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "customer_proxy_agent (to Customer Engagement Agent):\n",
            "\n",
            "Let's find something fun to read.\n",
            "Context: \n",
            "{'name': 'bob', 'location': 'new york'}\n",
            "bob from New York is interested in reading about food.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Customer Engagement Agent (to customer_proxy_agent):\n",
            "\n",
            "Hello Bob from New York! If you're interested in reading about food, did you know that New York City is known for having some of the best pizza in the world? It's a must-try if you're ever in the area! There are so many iconic pizzerias to choose from, each with their own delicious specialties. Maybe you can explore different pizza places and find your favorite slice! Enjoy your food adventures! ğŸ˜Š\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.summary)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWSpjcYGVWCO",
        "outputId": "818251f7-0bb7-4c50-84a7-fc9ffaa3d267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'bob', 'location': 'new york'}\n",
            "\n",
            "\n",
            "bob from New York is interested in reading about food.\n",
            "\n",
            "\n",
            "Bob from New York is interested in reading about food, and he should explore the diverse and delicious pizza options in the city.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.cost)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCd5ZQafVWFo",
        "outputId": "e766cd95-633f-450f-805e-7d696997275f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'usage_including_cached_inference': {'total_cost': 0.00013450000000000002, 'gpt-3.5-turbo-0125': {'cost': 0.00013450000000000002, 'prompt_tokens': 182, 'completion_tokens': 29, 'total_tokens': 211}}, 'usage_excluding_cached_inference': {'total_cost': 0.00013450000000000002, 'gpt-3.5-turbo-0125': {'cost': 0.00013450000000000002, 'prompt_tokens': 182, 'completion_tokens': 29, 'total_tokens': 211}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 4.8e-05, 'gpt-3.5-turbo-0125': {'cost': 4.8e-05, 'prompt_tokens': 63, 'completion_tokens': 11, 'total_tokens': 74}}, 'usage_excluding_cached_inference': {'total_cost': 4.8e-05, 'gpt-3.5-turbo-0125': {'cost': 4.8e-05, 'prompt_tokens': 63, 'completion_tokens': 11, 'total_tokens': 74}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 0.0003095, 'gpt-3.5-turbo-0125': {'cost': 0.0003095, 'prompt_tokens': 268, 'completion_tokens': 117, 'total_tokens': 385}}, 'usage_excluding_cached_inference': {'total_cost': 0.0003095, 'gpt-3.5-turbo-0125': {'cost': 0.0003095, 'prompt_tokens': 268, 'completion_tokens': 117, 'total_tokens': 385}}}\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}